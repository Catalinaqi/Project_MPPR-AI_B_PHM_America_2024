# -------------------------
# PIPELINE CONFIGURATION FILE
# CRISP-ML PRO Framework
# -------------------------
#
version: "1.0"
# -------------------------
# PIPELINE METADATA
# Objective: Define general information about the pipeline.
# -------------------------
pipeline:
  name: "classification_pipeline"
  task: "classification"
  objective: >
    End-to-end CRISP-ML pipeline for supervised classification
    (categorical target). Stages 2→5. Generates report artifacts (PNG/JSON)
    and persists trained models.
  # Variables injected by notebook (or resolved via dataset_config.yml)
  variables:
    #dataset_path: "${dataset_path}" # Path to the main dataset CSV file (e.g. train set)
    x_train_path: "${x_train_path}"   # Path to X_train CSV file (if separate)
    y_train_path: "${y_train_path}"   # Path to Y_train CSV file (if separate)
    x_test_path: "${x_test_path}"     # Path to X_test CSV file (if separate)
    x_validation_path: "${x_validation_path}" # Path to X_validation CSV file (if separate)

    #id_cols: "${id_cols}"           # Optional list of technical identifiers
    join_key: "id"    #"${join_key}"
    label_col: "faulty" #"${label_col}"  0/1 target for classification
    time_col: "${time_col}"           # Optional column name for time-based analyses (e.g. "event_time")

# -------------------------
# RUNTIME CONFIGURATION
# Objective: Define general runtime settings for the pipeline.
# -------------------------
runtime:
  random_seed: 42 # util seeding for reproducibility
  output_root: "out" # ${output_root}" # util base output dir for all artifacts
  overwrite_artifacts: true # util whether to overwrite existing outputs
  #log_level: "DEBUG"        # DEBUG | INFO | WARNING | ERROR | CRITICAL

# -------------------------
# PIPELINE STAGES CONFIGURATION
# Objective: Define each stage (2 to 5) with techniques, methods and parameters.
# -------------------------
stages:
  # -------------------------
  # STAGE 2 — DATA UNDERSTANDING (PHASE 2)
  # Phase focused on exploring data structure, semantics, and quality to identify issues and guide preprocessing.
  # -------------------------
  stage2_understanding:
    enabled: true
    objective: >
      Load CSV + describe structure + data quality assessment + EDA.
      Stage 2 MUST NOT modify data (report-only).
    dataset_input:
      source_type: "csv" # csv | json | database | external_system
      path:  "${x_train_path}" #"${dataset_path}"
      labels_input:
        source_type: "csv" # csv | json | database | external_system
        path: "${y_train_path}"
      read_strategy:
        mode: "sample"  # full | sample | chunked
        sample_rows: 200000
        random_state: 42
        chunk_size: 200000 # used only if mode=chunked
    output_policy:
      save_all_as_png: true
      figures_dir: "figures/stage2_understanding"
      tables_png_dir: "tables_png/stage2_understanding"
      dpi: 150
      max_rows: 60
      float_fmt: "{:.3f}"
      align: left
    steps:
      # 2.1 Data acquisition:
      # Process of collecting data and identifying their origin, sources, and generation context
      step_2_1_data_acquisition:
        enabled: true
        # Technique 1 - data_acquisition:
        # Process by which data are collected and made available for analysis from one or more sources (files, databases, APIs, external systems).
        technique: "data_acquisition"
        methods:
          # Method 1 - load_csv:
          # Loads structured data from delimited plain text files (CSV) into memory for analysis
          load_csv:
            enabled: true
            params:
              infer_datetime: false       # whether to attempt parsing date columns
              parse_dates: []          # e.g. ["event_time"] if known
      # 2.2 Describe data:
      # Analysis of the dataset structure and its basic characteristics, including schema, variables, and data types.
      step_2_2_describe_data:
        enabled: true
        techniques:
          # Technique 1 - describe_data:
          # Technique that numerically summarizes variables using measures of central tendency, dispersion, and range.
          descriptive_statistics:
            enabled: true
            methods:
              # Method 1 - describe:
              # Computes basic descriptive statistics for numerical variables.
              describe:
                enabled: true
                params:
                  include: "all"
                  numeric_only: false
              # Method 2 - min_max_mean_std:
              # Computes measures of range, central tendency, and dispersion.
              min_max_mean_std:
                enabled: true
                params:
                  numeric_only: true
          # Technique 2  - schema_inspection:
          # Analysis of the dataset structure, including columns, data types, cardinality, and the presence of null values.
          schema_inspection:
            enabled: true
            methods:
              # Method 1 - dtype_analysis:
              # Inspects the data types of each dataset column
              dtype_analysis:
                enabled: true
                #params: {}
              # Method 2 -cardinality_count:
              # Counts the number of unique values per column.
              cardinality_count:
                enabled: true
                params:
                  max_unique_to_report: 50
              # Method 3 -null_count:
              # Identifies and quantifies missing values per column.
              null_count:
                enabled: true

      # 2.3 Data quality assessment:
      # Identification of data quality issues such as missing values, inconsistencies, or anomalies, without applying corrective actions
      step_2_3_data_quality_assessment:
        enabled: true
        # Technique 4 - data_quality_assessment:
        # Systematic evaluation to detect data quality issues, without applying corrections yet.
        technique: "data_quality_assessment"
        methods:
          missing_analysis:
            enabled: true
            params:
              show_top_columns: 30
          outlier_detection:
            enabled: true
            params:
              method: "iqr"            # iqr | zscore
              iqr_k: 1.5
              zscore_threshold: 3.0
              numeric_only: true
              max_columns: 30
          duplicate_detection:
            enabled: true
            params:
              subset: null
              keep: "first"
          range_validation:
            enabled: false
            params:
              rules_file: "config/rules/ranges_quality_rules_config.yml"
              on_fail: "report_only"
          inconsistency_checks:
            enabled: false
            params:
              rules_file: "config/rules/logic_quality_rules_config.yml"
              on_fail: "report_only"
          percentile_analysis:
            enabled: true
            params:
              columns: [ "pa" ]
              percentiles: [ 0.01, 0.05, 0.95, 0.99, 0.999 ]


      # 2.4 EDA:
      # Systematic exploration of data to identify patterns, relationships, trends, and behavioral characteristics.
      step_2_4_eda:
        enabled: true
        # Technique 5 - exploratory_data_analysis (EDA):
        # Process of visual and analytical exploration to identify patterns, relationships, trends, and structures in the data.
        technique: "eda"
        methods:
          # Method 1 - histograms:
          # Visual representation of the distribution of numerical variables using bins.
          histograms:
            enabled: true
            params:
              numeric_only: true
              max_columns: 20
              bins: 30
          # Method 2 - box plots:
          # Represent dispersion, median, and potential outliers.
          boxplots:
            enabled: true
            params:
              numeric_only: true
              max_columns: 20
          # Method 3 - scatter_matrix:
          # Displays bivariate relationships among multiple variable
          scatter_matrix:
            enabled: false
            params:
              numeric_only: true
              max_columns: 8
              sample_rows: 10000
          # Method 4 - correlation_matrix:
          # Computes statistical correlation between numerical variables.
          correlation_matrix:
            enabled: true
            params:
              method: "pearson"        # pearson | spearman
              numeric_only: true
              max_columns: 30
          # Method 5 - pca_exploratory:
          # Reduces dimensionality to visualize the global structure of the dataset.
          pca_exploratory:
            enabled: false
            params:
              numeric_only: true
              n_components: 2
              sample_rows: 20000


  # -------------------------
  # STAGE 3 — DATA PREPARATION (PHASE 3)
  # Phase where data are cleaned, transformed, integrated, and formatted for modeling.
  # -------------------------
  stage3_preparation:
    enabled: true
    objective: >
      Prepare data for classification: selection + cleaning + transformations +
      (optional integration) + split + formatting. Generates artifacts.
    output_policy:
      save_all_as_png: true
      figures_dir: "figures/stage3_preparation"
      tables_png_dir: "tables_png/stage3_preparation"
    steps:
      # 3.1 Data selection
      # Process of deciding which data are included in the analysis and which are excluded based on relevance and quality criteria.
      step_3_1_data_selection:
        enabled: true
        techniques:
          # Technique 1 - data_selection:
          # Definition of the dataset scope, determining which rows and columns are included in the analysis.
          dataset_definition:
            enabled: true
            methods:
              # Method 1 - manual_include_exclude:
              # Explicit selection of relevant columns to include or exclude from the analysis.
              manual_include_exclude:
                enabled: false
                params: { include: [], exclude: [] }
              # Method 2 - drop_technical_columns:
              # Removal of technical or non-informative columns (e.g. IDs, logs, metadata).
              drop_technical_columns:
                enabled: false
                params:
                  patterns: ["id", "uuid", "log"]
              # Method 3 - population_filtering:
              # Filtering of records according to business or analytical rules.
              population_filtering:
                enabled: false
                params:
                  rules_file: "config/rules/logic_quality_rules_config.yml"
          # Technique 2 - feature_selection:
          # Selection of informative variables and removal of redundant features.
          feature_selection:
            enabled: true
            methods:
              # Method 1 - variance_threshold:
              # Feature selection based on domain knowledge and semantic relevance.
              semantic_based_selection:
                enabled: false
                params: { keep: [] }
              # Method 2 - correlation_filtering:
              # Application of expert-defined inclusion and exclusion rules.
              business_rule_filtering:
                enabled: false
                params:
                  rules_file: "config/rules/logic_quality_rules_config.yml"
              # Method 3 - remove_constant_features:
              # Elimination of variables with no variance.
              remove_constant_features:
                enabled: true
                params: { threshold_unique: 1 }
              # Method 4 - remove_duplicate_features:
              # Removal of redundant or duplicated columns.
              remove_duplicate_features:
                enabled: true
                params: { strategy: "exact" }

      # 3.2 Data cleaning
      # Correction of errors, inconsistencies, and noise in the data, without introducing new information
      step_3_2_data_cleaning:
        enabled: true
        techniques:
          # Technique 1 - Missing data handling:
          # Management and treatment of missing values.
          missing_data_handling:
            enabled: true
            # Ensures you do NOT apply multiple numeric imputations at once.
            selection_policy:
              numeric_imputer: "median_imputation"     # mean_imputation | median_imputation | knn_imputation | mice_imputation
            methods:
              # Method 1 - median_imputation:
              # Simple statistical imputation using the median of the variable.
              median_imputation:
                enabled: true
                params: { numeric_only: true }

      # 3.3 Data transformation
      # Modification of data representation to improve model learning and performance (e.g., scaling, encoding, normalization).
      step_3_3_data_transformation:
        enabled: true
        techniques:
          # Technique 1 - feature_scaling:
          # Rescaling of numerical variables to ensure comparable magnitudes.
          feature_scaling:
            enabled: true
            selection_policy:
              scaling_method: "standard_scaling"       # standard_scaling | minmax_scaling | robust_scaling | maxabs_scaling | none
            methods:
              # Method 1 - standard_scaling:
              # Transforms data to have zero mean and unit standard deviation.
              standard_scaling: { enabled: true, params: {} }
              # Method 2 - robust_scaling:
              # Scaling using statistics that are robust to outliers (median and IQR).
              robust_scaling: { enabled: false, params: {} }
          # Technique 2 - categorical_encoding:
          # Conversion of categorical variables into numerical representations understandable by ML algorithms
          encoding:
            enabled: false
            selection_policy:
              categorical_encoding: "one_hot_encoding" # one_hot_encoding | ordinal_encoding | frequency_encoding
            methods:
              # Method 1 - one_hot_encoding:
              # Creation of binary columns for each category level.
              one_hot_encoding:
                enabled: true
                params: { handle_unknown: "ignore" }
              # Method 2 - ordinal_encoding:
              # Assignment of integer values to categories based on a defined order or frequency.
              ordinal_encoding:
                enabled: false
                params: {}
              # Method 3 - frequency_encoding:
              # Replacement of categories with their corresponding frequency counts.
              frequency_encoding:
                enabled: false
                params: { min_freq: 1 }
          # Technique 3 - feature_engineering:
          # Creation of new features from existing ones to capture implicit or hidden information.
          feature_engineering:
            enabled: false
            methods:
              # Method 1 - ratios:
              ratios:
                enabled: false
                params: { definitions: [] }
              # Method 2 - aggregations:
              aggregations:
                enabled: false
                params: { groupby_cols: [], agg_map: {} }
              # Method 3 - interactions:
              interactions:
                enabled: false
                params: {}
             # Method 4- polynomial_features:
              polynomial_features:
                enabled: false
                params: { degree: 2 }


      # 3.4 Data integration (optional)
      # Combination of multiple data sources into a single coherent, consistent, and aligned dataset.
      step_3_4_data_integration:
        enabled: false

      # 3.5 Data formatting (split + final shapes)
      # Preparation of data in the exact format required by the algorithm, without altering its informational content.
      step_3_5_data_formatting:
        enabled: true
        techniques:
          # Technique 1 - data_splitting:
          # Separation of the dataset into training, validation, and test subsets.
          data_split:
            enabled: true
            selection_policy:
              split_method: "holdout"                 # holdout | train_val_test
            methods:
              # Method 1 -  holdout
              # Splits the dataset into training and test sets.
              holdout:
                enabled: true
                params:
                  test_size: 0.2
                  stratify: true
                  random_state: 42
              # Method 2 - train_val_test
              # Splits the dataset into three independent subsets.
              train_val_test:
                enabled: false
                params:
                  train: 0.7
                  val: 0.1
                  test: 0.2
                  stratify: true
                  random_state: 42
          # Technique 2 - data_formatting:
          # Adaptation of data structure and types to the exact format required by ML algorithms, without altering informational content.
          dataset_formatting:
            enabled: true
            methods:
              # Method 1 - join_x_y:
              # Integration of separate feature (X) and target (Y) datasets into a single dataset for modeling.
              join_x_y:
                enabled: true
                params:
                  x_path: "${x_train_path}"
                  y_path: "${y_train_path}"
                  join_key: "${join_key}"
                  label_col: "${label_col}"
                  drop_after_join: ["trq_margin","id"] # example of a column to drop after the join, if it exists; can be empty or omitted
              # Method 2 - type_casting:
              # Explicit conversion of data types.
              type_casting:
                enabled: true
                params: {}
              # Method 3 - sparse_dense_format:
              # Conversion between sparse and dense matrix representations.
              sparse_dense_format:
                enabled: true
                params: { format: "sparse" }          # sparse | dense
              # Method 4 - x_y_separation:
              # Separation of predictor variables (features) and the target variable.
              x_y_separation:
                enabled: true
                params: {}


  # -------------------------
  # STAGE 4 — MODELING (PHASE 4)
  # Phase dedicated to selecting, training, and tuning models to learn patterns from data.
  # -------------------------
  stage4_modeling:
    enabled: true
    objective: >
      Phase 4 (Modeling) for classification: algorithm selection, training,
      test design (CV/holdout), hyperparameter tuning, and evaluation metrics.
    output_policy:
      models_dir: "models"
      figures_dir: "figures/stage4_modeling"
      tables_png_dir: "tables_png/stage4_modeling"
      metrics_file: "metrics.json"
      save_all_as_png: true
    steps:
      # 4.1 Select the technique (Algorithm selection)
      # Choose the appropriate algorithm.
      step_4_1_algorithm_selection:
        enabled: true
        technique: "algorithm_selection"
        methods:
          # Method 1 - decision_tree_classifier:
          decision_tree_classifier:
            enabled: true
            params:
              random_state: 42
          # Method 2 - random_forest_classifier:
          random_forest_classifier:
            enabled: true
            params:
                n_estimators: 100
                random_state: 42
                n_jobs: -1
          # Method 3 - svm_classifier:
          svm_classifier:
            enabled: false
            params: {}
          # Method 4 - naive_bayes_classifier:
          naive_bayes_classifier:
            enabled: false
            params: {}
          # Method 5 - knn_classifier:
          knn_classifier:
            enabled: false
            params: {}

      # 4.2 Build the model (Model training)
      # Train the model.
      step_4_2_model_training:
        enabled: true
        technique: "model_training"
        methods:
          # Method 1 - model_fit:
          fit:
            enabled: true
            params:
              fit_best_only: true          # keep best model as "best_model"
              store_all_models: true       # persist all trained models
          # Method 2 - hyperparameter_tuning:
          hyperparameter_tuning:
            enabled: false                  # "se puede aplicar"
            params:
              strategy: "grid"             # grid | random
              scoring: "f1_weighted"       # classification-oriented scoring
              n_iter: 30                   # used only if strategy=random
              refit: true
            grids:
              decision_tree_classifier:
                max_depth: [3, 5, 10, null]
                min_samples_split: [2, 5, 10]
              random_forest_classifier:
                n_estimators: [100, 200]
                max_depth: [null, 10, 20]
                min_samples_split: [2, 5]
              svm_classifier:
                C: [0.1, 1, 10]
                kernel: ["rbf", "linear"]
              naive_bayes_classifier:
                var_smoothing: [1.0e-09, 1.0e-08]
              knn_classifier:
                n_neighbors: [3, 5, 7, 11]
                weights: ["uniform", "distance"]

      # 4.3 Generate the test project (Test design)
      # Define how the model will be tested and validated.
      step_4_3_test_design:
        enabled: true
        technique: "test_design"
        methods:
          # Method 1 - cross_validation:
          # Cross-validation applies to classification (per your table)
          cross_validation:
            enabled: false # "se puede aplicar"
            params:
              cv_folds: 5
              shuffle: true
              random_state: 42
          # Method 2 - holdout:
          # Hold-out applies to classification (per your table)
          holdout:
            enabled: true
            params:
              test_size: 0.2
              stratify: true
              random_state: 42

      # 4.4 Evaluate the model (Model evaluation)
      # Evaluate model performance using appropriate metrics.
      step_4_4_model_evaluation:
        enabled: true
        technique: "model_evaluation"
        metrics:
          accuracy:
            enabled: true
            params: {}
          f1_score:
            enabled: true
            params:
              average: "weighted"



  # -------------------------
  # STAGE 5 — EVALUATION & INTERPRETATION (PHASE 5)
  # Phase where model performance is evaluated, results are interpreted, and business value is assessed.
  # -------------------------
  stage5_evaluation_and_interpretation:
    enabled: true
    objective: >
      Phase 5 (Evaluation & Interpretation): translate model results into
      actionable knowledge, validate business value, audit the pipeline,
      and decide next steps.
    output_policy:
      figures_dir: "figures/stage5_evaluation_and_interpretation"
      tables_png_dir: "tables_png/stage5_evaluation_and_interpretation"
      metrics_file: "metrics.json"
      save_all_as_png: true
    steps:
      # 5.1 Extract knowledge (Interpretation)
      # Understand what the model has learned and why.
      step_5_1_interpretation:
        enabled: true
        technique: "interpretation"
        methods:
          # Method 1 - feature_importance:
          # Measures the relative contribution of each feature to the model.
          feature_importance:
            enabled: true
            params:
              top_k: 30
              normalize: true
          # Method 2 - coefficients:
          # Direct interpretation of feature weights in linear models.
          coefficients:
            enabled: false
            params:
              top_k: 30
              include_models: ["svm_linear", "logistic_like_if_any"]  # safe placeholder
          # Method 3 - permutation_importance:
          # Measures performance loss when a feature’s values are randomly permuted.
          permutation_importance:
            enabled: true
            params:
              n_repeats: 10
              random_state: 42
          # Method 5 - confusion_matrix_analysis:
          # Detailed analysis of correct and incorrect predictions by class.
          confusion_matrix_analysis:
            enabled: true
            params:
              normalize: "true"          # true | pred | all | null
              include_class_report: true

      # 5.2 Evaluate results (Business evaluation)
      # Assess whether the model delivers real business value
      step_5_2_business_evaluation:
        enabled: true
        technique: "business_evaluation"
        methods:
          # Method 1 - compare_models:
          # Comparison among multiple candidate models.
          compare_models:
            enabled: true
            params:
              rank_by: ["f1_weighted", "accuracy"]
          # Method 2 - baseline_comparison:
          # Comparison against a simple or current baseline solution.
          baseline_comparison:
            enabled: true
            params:
              baseline: "majority_class" # majority_class | random | custom
          # Method 3 - business_kpis:
          # Translation of technical metrics into economic or business impact indicators.
          business_kpis:
            enabled: false
            params:
              kpi_rules_file: "config/rules/business_kpi_rules_config.yml"
              currency: "EUR"

      # 5.3 Review the process (Process audit)
      # Verify that the process was correct, consistent, and reproducible.
      step_5_3_process_audit:
        enabled: true
        technique: "process_audit"
        methods:
          # Method 1 - pipeline_review:
          # Manual review of the entire processing pipeline.
          pipeline_review:
            enabled: false
            params:
              checklist_file: "config/rules/pipeline_review_checklist.yml"  # optional
          # Method 2 - reproducibility_check:
          # Verification that results are repeatable and reproducible.
          reproducibility_check:
            enabled: true
            params:
              require_seed: true
              require_config_snapshot: true
          # Method 3 - data_drift_detection:
          # Detection of information leakage from training to test data.
          leakage_detection:
            enabled: true
            params:
              label_col: "${label_col}"
              #target_col: "${target_col}"
              check_time_leakage: false   # classification default
              time_col: "${time_col}"

      # 5.4 Determine next steps (Decision making)
      # Decide how to proceed with the model.
      step_5_4_decision_making:
        enabled: false
        technique: "decision_making"
        methods:
          # Method 1 - go_no_go_decision:
          # Decision on whether to adopt or reject the model.
          go_no_go_decision:
            enabled: true
            params:
              min_accuracy: 0.70
              min_f1_weighted: 0.65
          # Method 2 - iterate_model:
          # Start a new modeling iteration to improve performance or robustness.
          iterate_model:
            enabled: true
            params:
              enable_backloop_to_stage: 4
          # Method 3 - human_in_the_loop:
          # Human validation in critical or high-risk decisions.
          human_in_the_loop:
            enabled: true
            params:
              require_manual_signoff: true


      step_5_5_submission_export:
          enabled: false
          technique: "submission_export"
          params:
            output_dir: "submissions" # directory where submission files will be saved
            test_filename: "submission.jso" # name of the submission file for test set predictions (e.g. "submission.csv" or "submission.json")
            validation_filename: "validation_submission.jso" # name of the submission file for validation set predictions (e.g. "validation_submission.csv" or "validation_submission.json")
            id_field: "id"   # name of the identifier field to include in the submission file (e.g. "id", "uuid", "record_id")
